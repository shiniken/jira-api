{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # handles the HTTP requests to JIRA's REST API\n",
    "import re  # used for regular expression operation\n",
    "import pandas as pd  # data manipulation\n",
    "from requests.auth import HTTPBasicAuth  # auth module for secure API requests\n",
    "from datetime import datetime # used for date calc post-processing\n",
    "\n",
    "# API req credentials\n",
    "# HTTPBasicAuth is used to encode the auth sent\n",
    "# ensures (to the API) that the request is from an authorized user\n",
    "user = 'intentionally_not_here'  \n",
    "api_token = 'also_intentionally_not_here'\n",
    "auth = HTTPBasicAuth(user, api_token)\n",
    "\n",
    "# jql: string defines the query for JIRA's API using JQL\n",
    "# fields: the specific fields that JIRA will return for each issue\n",
    "jql = 'project=your_project'\n",
    "fields = 'summary,etc.'\n",
    "\n",
    "# endpoint - this URL is the entry point to JIRA's REST API for searching issues based on the JQL defined earlier\n",
    "url = \"https://yourcompany.atlassian.net/rest/api/3/search\"\n",
    "\n",
    "# function uses a regular expression to search for a tier number within the issue summary text\n",
    "# the re.IGNORECASE flag makes the search case-insensitive\n",
    "def extract_tier(summary):\n",
    "    match = re.search(r'Tier[^0-9]*?(\\d+)', summary, re.IGNORECASE)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# function converts a duration in seconds into a more readable format (days, hours, minutes)\n",
    "# uses integer division and modulo operations to break down the total seconds into components\n",
    "def format_time_spent(seconds):\n",
    "    \"\"\"formats time spent from seconds into a string of days / hrs / minutes\"\"\"\n",
    "    \n",
    "    # convert string to integer, in case it's passed as a string\n",
    "    seconds = int(seconds)\n",
    "    \n",
    "    # time periods\n",
    "    minute = 60\n",
    "    hour = 60 * minute\n",
    "    day = 24 * hour\n",
    "\n",
    "    if seconds < hour:\n",
    "        return f\"{seconds // minute}m\"\n",
    "    elif seconds < day:\n",
    "        return f\"{seconds // hour}h {seconds % hour // minute}m\"\n",
    "    else:\n",
    "        days_spent = seconds // day\n",
    "        hours_spent = (seconds % day) // hour\n",
    "        return f\"{days_spent}d {hours_spent}h\"\n",
    "\n",
    "# fetch all issues from JIRA API\n",
    "# makes paginated requests (requests in chunks) until all issues matching the JQL are retrieved\n",
    "# uses a loop to handle pagination, adjusting the start_at parameter each time to fetch the next page of results\n",
    "def fetch_issues(url, auth, jql, fields):\n",
    "    issues = []\n",
    "    start_at = 0\n",
    "    max_results = 100\n",
    "    total = None\n",
    "    with requests.Session() as session:\n",
    "        session.auth = auth\n",
    "        while total is None or len(issues) < total:\n",
    "            params = {\n",
    "                'jql': jql,\n",
    "                'fields': fields,\n",
    "                'maxResults': max_results,\n",
    "                'startAt': start_at,\n",
    "                'expand': 'changelog'  # includes changelog in the response for history\n",
    "            }\n",
    "            response = session.get(url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                total = data['total']\n",
    "                issues.extend(data['issues'])\n",
    "                start_at += len(data['issues'])\n",
    "                print(f\"fetched {len(data['issues'])} issues, total so far: {len(issues)} out of {total}\")\n",
    "            else:\n",
    "                print(f\"failed to fetch data: {response.status_code} - {response.reason}\")\n",
    "                break\n",
    "    return issues\n",
    "\n",
    "# function fetches all worklogs for a specific issue, accounting for the possibility that worklogs may also be paginated\n",
    "def fetch_all_worklogs_for_issue(session, issue_key):\n",
    "    worklog_url = f\"https://anfcorp.atlassian.net/rest/api/3/issue/{issue_key}/worklog\"\n",
    "    all_worklogs = []\n",
    "    start_at = 0\n",
    "    while True:\n",
    "        response = session.get(worklog_url, params={'startAt': start_at})\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            all_worklogs.extend(data['worklogs'])\n",
    "            if start_at + len(data['worklogs']) < data['total']:\n",
    "                start_at += len(data['worklogs'])\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            print(f\"failed to fetch worklogs: {response.status_code} - {response.reason}\")\n",
    "            break\n",
    "    return all_worklogs\n",
    "\n",
    "# this function iterates over each issue retrieved by fetch_issues to:\n",
    "# extract specific fields from the issue\n",
    "# fetch all worklogs using fetch_all_worklogs_for_issue\n",
    "# process the changelog to determine incremental updates to the time spent on each issue\n",
    "# the time spent is calculated based on the difference in seconds from the changelog entries, only recording changes (incremental time) rather than total time spent\n",
    "def extract_worklogs(issues):\n",
    "    data_rows = []\n",
    "    with requests.Session() as session:  # reuses the session for multiple requests\n",
    "        session.auth = auth\n",
    "        for issue in issues:\n",
    "            issue_key = issue['key']\n",
    "            issue_fields = issue['fields']\n",
    "            tier = extract_tier(issue_fields['summary'])\n",
    "            components = [comp['name'] for comp in issue_fields.get('components', [])]\n",
    "            issue_data = {\n",
    "                'Key': issue_key,\n",
    "                'Summary': issue_fields['summary'],\n",
    "                'Issue_Type': issue_fields['issuetype']['name'],\n",
    "                'Status': issue_fields['status']['name'],\n",
    "                'Creator': issue_fields['creator']['displayName'],\n",
    "                'Assignee': issue_fields['assignee']['displayName'] if issue_fields.get('assignee') else 'Unassigned',\n",
    "                'Reporter': issue_fields['reporter']['displayName'] if issue_fields.get('reporter') else 'Unknown',\n",
    "                'Created': issue_fields['created'],\n",
    "                'Updated': issue_fields['updated'],\n",
    "                'Labels': ', '.join(issue_fields['labels']),\n",
    "                'Components': ', '.join(components),\n",
    "                'Tier': tier\n",
    "            }\n",
    "            \n",
    "            # fetch all worklogs if they are paginated\n",
    "            all_worklogs = fetch_all_worklogs_for_issue(session, issue_key)\n",
    "            if not all_worklogs:\n",
    "                data_rows.append(issue_data)\n",
    "            for log in all_worklogs:\n",
    "                time_spent_formatted = format_time_spent(log['timeSpentSeconds'])\n",
    "                log_data = issue_data.copy()\n",
    "                log_data.update({\n",
    "                    'Worker': log['author']['displayName'],\n",
    "                    'Time_Spent': time_spent_formatted,\n",
    "                    'Worklog_Created': log['started'],\n",
    "                    'Log_Type': 'Work Log'\n",
    "                })\n",
    "                data_rows.append(log_data)\n",
    "\n",
    "            # processes the changelog to find updates to time spent\n",
    "            for history in issue.get('changelog', {}).get('histories', []):\n",
    "                for item in history['items']:\n",
    "                    if item['field'] == 'timespent' or item['field'] == 'Time Spent':\n",
    "                        \n",
    "                        # converts changelog timespent from and to strings to seconds\n",
    "                        from_seconds = int(item['fromString'] or 0)  # if 'fromString' is None, default to 0\n",
    "                        to_seconds = int(item['toString'] or 0)  # if 'toString' is None, default to 0\n",
    "                        \n",
    "                        # calc incremental time spent\n",
    "                        timespent_change_seconds = to_seconds - from_seconds\n",
    "                        \n",
    "                        # skips if no change in time spent\n",
    "                        if timespent_change_seconds <= 0:\n",
    "                            continue\n",
    "                        \n",
    "                        # formatting the incremental time spent\n",
    "                        time_spent_formatted = format_time_spent(timespent_change_seconds)\n",
    "                        time_spent_entry = issue_data.copy()\n",
    "                        time_spent_entry.update({\n",
    "                            'Worker': history['author']['displayName'],\n",
    "                            'Time_Spent': time_spent_formatted,\n",
    "                            'Worklog_Created': history['created'],\n",
    "                            'Log_Type': 'Time Spent Update'\n",
    "                        })\n",
    "                        data_rows.append(time_spent_entry)\n",
    "    return data_rows\n",
    "\n",
    "# calls fetch_issues to get all issues and then processes each issue with extract_worklogs to extract worklogs and time spent updates\n",
    "issues = fetch_issues(url, auth, jql, fields)\n",
    "data_rows = extract_worklogs(issues)\n",
    "\n",
    "# adds a 'Worklog_Created_Date' key only if 'Worklog_Created' is present\n",
    "for row in data_rows:\n",
    "    if 'Worklog_Created' in row:\n",
    "        # extracts the date part from the 'Worklog_Created' timestamp\n",
    "        # uses datetime.strptime to parse the timestamp into a datetime object and then takes the date part\n",
    "        row['Worklog_Created_Date'] = datetime.strptime(row['Worklog_Created'], \"%Y-%m-%dT%H:%M:%S.%f%z\").date()\n",
    "\n",
    "# identifies the \"Time Spent Update\" entries to remove\n",
    "rows_to_remove = set()\n",
    "for work_log in filter(lambda d: d.get('Log_Type') == 'Work Log' and 'Worklog_Created' in d, data_rows):\n",
    "    for time_spent_update in filter(lambda d: d.get('Log_Type') == 'Time Spent Update' and 'Worklog_Created' in d, data_rows):\n",
    "        \n",
    "        # nested loop checks each 'Work Log' against each 'Time Spent Update'\n",
    "        # uses the get method to access 'Log_Type' and ensures 'Worklog_Created' exists\n",
    "        # if they are by the same worker on the same date, it marks the 'Time Spent Update' for removal (work logs are prioritized)\n",
    "        if (work_log['Worker'] == time_spent_update['Worker'] and\n",
    "            work_log['Worklog_Created_Date'] == time_spent_update['Worklog_Created_Date']):\n",
    "            rows_to_remove.add(time_spent_update['Worklog_Created'])\n",
    "\n",
    "# retains all original rows, removing only the duplicates identified\n",
    "final_data_rows = []\n",
    "for row in data_rows:\n",
    "\n",
    "    # rows that are 'Work Log' type are always kept\n",
    "    # 'Time Spent Update' rows are kept only if they're not marked for removal\n",
    "    # if no 'Log_Type' or 'Worklog_Created' keys -> keep the row\n",
    "    if 'Log_Type' not in row or 'Worklog_Created' not in row:\n",
    "        final_data_rows.append(row)\n",
    "    # if 'Log_Type' is 'Work Log' -> keep the row\n",
    "    elif row['Log_Type'] == 'Work Log':\n",
    "        final_data_rows.append(row)\n",
    "    # if 'Log_Type' is 'Time Spent Update' + it's not in the rows to remove -> keep the row\n",
    "    elif row['Log_Type'] == 'Time Spent Update' and row['Worklog_Created'] not in rows_to_remove:\n",
    "        final_data_rows.append(row)\n",
    "\n",
    "# pandas df from the processed data and writes it to a csv file\n",
    "# index=False is used so that pandas doesn't write the df index as a separate column in the csv output\n",
    "df = pd.DataFrame(final_data_rows)\n",
    "\n",
    "# path to fabric lakehouse\n",
    "out_path = \"/lakehouse/default/Files/your_file_name.csv\"\n",
    "\n",
    "# csv\n",
    "df.to_csv(out_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
